{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM算法梳理\n",
    "\n",
    "1. LightGBM\n",
    "- LightGBM的起源\n",
    "- Histogram VS pre-sorted\n",
    "- leaf-wise VS level-wise\n",
    "- 特征并行和数据并行\n",
    "- 顺序访问梯度\n",
    "- 支持类别特征\n",
    "- 应用场景\n",
    "- sklearn参数\n",
    "- CatBoost(了解)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.LightGBM起源\n",
    "GBDT在面对大数据(in terms of both the number of features\n",
    "and the number of instances)的时候，因为要对每个特征遍历整个数据集计算信息增益(information gain)，所以会非常耗时(time consuming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决以上问题，提出了两种方法\n",
    "- Gradient-based One-Side Sampling (GOSS). \n",
    "较大梯度对于信息增益有正影响；保留梯度大的样本，随即删除梯度小的样本。\n",
    "- Exclusive Feature Bundling (EFB).\n",
    "通常在实际应用中，虽然有大量的特征，但特征空间是相当稀疏的(sparse)，这为我们设计一种近乎无损的方法来减少有效特征的数量提供了可能。具体地说，在一个稀疏的特征空间(future space)中，许多特征(几乎)是独占的，即，它们很少同时取非零值。为此，我们设计了一种有效的算法，将最优捆绑(optimal bundling)问题简化为图着色（graph coloring）问题(如果两个特征不互斥，则以特征为顶点，每两个特征边相加)，并用一个常数近似比（constant approximation ratio）的贪婪算法求解。\n",
    "\n",
    "我们称这种新的基于GOSS和EFB的GBDT算法为LightGBM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Histogram VS pre-sorted\n",
    "\n",
    "GBDT是通过拟合残差来学习决策树；GBDT中最耗时的就是在学习决策树过程中寻找最好分割点。\n",
    "\n",
    "- 最流行的算法之一就是预排序(pre-sorted)算法，它为每个特性值预先排序并枚举所有可能的拆分点。这种算法可以精确地找到最佳分割点。但是在训练速度和内存消耗上都inefficient.而且可能存在过拟合。\n",
    " - 可能会造成cache miss\n",
    "   - 对梯度的访问，计算gain的时候会访问梯度，不同feature访问梯度的顺序是不一样，随机的\n",
    "   - 行号和叶子节点的索引表，防止切分的时候对所有数据切分，和上面一样随机访问\n",
    "![](https://i.loli.net/2019/08/14/hYytRP49soneuWX.jpg)\n",
    "- 另一种方法就是histogram-based algorithm，基于直方图算法，这种算法在训练的时候将连续的特征值，分为离散的bins，并用它们构建新的特征直方图。且用更小的byte来存储bins。用直方图做差来得到加速效果。\n",
    " - 对梯度访问:不需要对feature排序，所有feature都用同样的方式访问，对梯度访问顺序进行排序。\n",
    " - 直方图不需要数据id到叶子号的索引，没有cache miss的问题。在数据量很大的时候顺序访问比随机访问效率大很多\n",
    "![](https://i.loli.net/2019/08/14/FlCu2txBYdD91HP.jpg)\n",
    "所以这种算法更在速度和内存消耗上都更高效。\n",
    "\n",
    " - It costs $O(#data × #feature)$ for histogram building and $O(#bin × #feature)$ for\n",
    "split point finding.Since $#bin$ is usually much smaller than #data, histogram building will dominate\n",
    "the computational complexity. If we can reduce $#data$ or #feature, we will be able to substantially\n",
    "speed up the training of GBDT.\n",
    "\n",
    "- 传统的ml算法，不能直接输入类别特征，需要对数据离散化，转化成多维的0/1特征，降低效率。lightgbm通过更改决策树的决策规则，直接支持输入类别特征，不用离散化。\n",
    "![](https://i.loli.net/2019/08/14/5Tj9H8EVJOQDuZm.jpg)\n",
    "\n",
    "- histogram 算法不能找到很精确的分割点，训练误差没有 pre-sorted 好。但从实验结果来看， histogram 算法在测试集的误差和 pre-sorted 算法差异并不是很大，甚至有时候效果更好。实际上可能决策树对于分割点的精确程度并不太敏感，而且较“粗”的分割点也自带正则化的效果，再加上boosting算法本身就是弱分类器的集成。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.leaf-wise VS level-wise\n",
    "lgb带有深度限制的leaf-wise来提高模型精度，且比level-wise高效。叶数量一样的时候leaf-wise可以减少更多的loss，得到更好的精度；但可能会过拟合，比如在小数据集训练，产生比较深的决策树，所以加了深度限制。\n",
    "\n",
    "- 生长策略\n",
    "\n",
    "  - Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。\n",
    "  ![](https://github.com/microsoft/LightGBM/raw/master/docs/_static/images/level-wise.png)\n",
    "\n",
    "  - Leaf-wise则是一种更为高效的策略：每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。对于过拟合，可以通过设置另一个参数 max-depth 来控制它分裂产生的树的最大深度。\n",
    "  ![](https://github.com/microsoft/LightGBM/raw/master/docs/_static/images/leaf-wise.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.特征并行和数据并行\n",
    "![](https://i.loli.net/2019/08/14/ZOVbz1xS4Ncnjd2.jpg)\n",
    "![](https://i.loli.net/2019/08/14/ai15HPMZ6Cc2ugU.jpg)\n",
    "\n",
    "![](https://i.loli.net/2019/08/14/IVwn7QtDeghLZ4J.jpg)\n",
    "\n",
    "Optimization in Parallel Learning\n",
    "---------------------------------\n",
    "\n",
    "LightGBM provides the following parallel learning algorithms.\n",
    "\n",
    "Feature Parallel\n",
    "~~~~~~~~~~~~~~~~\n",
    "\n",
    "Traditional Algorithm\n",
    "^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Feature parallel aims to parallelize the \"Find Best Split\" in the decision tree. The procedure of traditional feature parallel is:\n",
    "\n",
    "1. Partition data vertically (different machines have different feature set).\n",
    "\n",
    "2. Workers find local best split point {feature, threshold} on local feature set.\n",
    "\n",
    "3. Communicate local best splits with each other and get the best one.\n",
    "\n",
    "4. Worker with best split to perform split, then send the split result of data to other workers.\n",
    "\n",
    "5. Other workers split data according to received data.\n",
    "\n",
    "The shortcomings of traditional feature parallel:\n",
    "\n",
    "-  Has computation overhead, since it cannot speed up \"split\", whose time complexity is ``O(#data)``.\n",
    "   Thus, feature parallel cannot speed up well when ``#data`` is large.\n",
    "\n",
    "-  Need communication of split result, which costs about ``O(#data / 8)`` (one bit for one data).\n",
    "\n",
    "Feature Parallel in LightGBM\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Since feature parallel cannot speed up well when ``#data`` is large, we make a little change: instead of partitioning data vertically, every worker holds the full data.\n",
    "Thus, LightGBM doesn't need to communicate for split result of data since every worker knows how to split data.\n",
    "And ``#data`` won't be larger, so it is reasonable to hold the full data in every machine.\n",
    "\n",
    "The procedure of feature parallel in LightGBM:\n",
    "\n",
    "1. Workers find local best split point {feature, threshold} on local feature set.\n",
    "\n",
    "2. Communicate local best splits with each other and get the best one.\n",
    "\n",
    "3. Perform best split.\n",
    "\n",
    "However, this feature parallel algorithm still suffers from computation overhead for \"split\" when ``#data`` is large.\n",
    "So it will be better to use data parallel when ``#data`` is large.\n",
    "\n",
    "Data Parallel\n",
    "~~~~~~~~~~~~~\n",
    "\n",
    "Traditional Algorithm\n",
    "^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Data parallel aims to parallelize the whole decision learning. The procedure of data parallel is:\n",
    "\n",
    "1. Partition data horizontally.\n",
    "\n",
    "2. Workers use local data to construct local histograms.\n",
    "\n",
    "3. Merge global histograms from all local histograms.\n",
    "\n",
    "4. Find best split from merged global histograms, then perform splits.\n",
    "\n",
    "The shortcomings of traditional data parallel:\n",
    "\n",
    "-  High communication cost.\n",
    "   If using point-to-point communication algorithm, communication cost for one machine is about ``O(#machine * #feature * #bin)``.\n",
    "   If using collective communication algorithm (e.g. \"All Reduce\"), communication cost is about ``O(2 * #feature * #bin)`` (check cost of \"All Reduce\" in chapter 4.5 at `[9] <#references>`__).\n",
    "\n",
    "Data Parallel in LightGBM\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "We reduce communication cost of data parallel in LightGBM:\n",
    "\n",
    "1. Instead of \"Merge global histograms from all local histograms\", LightGBM uses \"Reduce Scatter\" to merge histograms of different (non-overlapping) features for different workers.\n",
    "   Then workers find the local best split on local merged histograms and sync up the global best split.\n",
    "\n",
    "2. As aforementioned, LightGBM uses histogram subtraction to speed up training.\n",
    "   Based on this, we can communicate histograms only for one leaf, and get its neighbor's histograms by subtraction as well.\n",
    "\n",
    "All things considered, data parallel in LightGBM has time complexity ``O(0.5 * #feature * #bin)``.\n",
    "\n",
    "Voting Parallel\n",
    "~~~~~~~~~~~~~~~\n",
    "\n",
    "Voting parallel further reduces the communication cost in `Data Parallel <#data-parallel>`__ to constant cost.\n",
    "It uses two-stage voting to reduce the communication cost of feature histograms\\ `[10] <#references>`__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.顺序访问梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.支持类别特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.sklearn参数\n",
    "https://juejin.im/post/5b76437ae51d45666b5d9b05\n",
    "https://lightgbm.apachecn.org/#/docs/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.CatBoost(了解)\n",
    "http://learningsys.org/nips17/assets/papers/paper_11.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert max_depth 2 num_leaves\n",
    "\n",
    "num_leaves = 2^max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
