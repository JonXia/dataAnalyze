# datawhale 11 月组队学习 模型减肥秘籍——模型剪枝

> datawhale课程链接：[模型减肥秘籍：模型压缩技术-课程详情 | Datawhale](https://www.datawhale.cn/learn/content/68/960)

## **为何可剪枝**：

可以通过识别并移除那些对模型性能影响较小的参数，从而减少模型的复杂性和计算成本

- 理论：彩票假说
  - 该假说认为，在随机初始化的大型神经网络中，存在一个子网络独立训练后也能与完整网络达到相似性能，表明并非所有部分都对最终性能至关重要，从而支持了剪枝的理论基础

- 模型稀疏性：
  - 网络稀疏性研究表明，许多深度神经网络的参数大多接近于零，这启发了剪枝技术，通过移除这些非显著的参数来简化模型。
  - L1正则化则进一步鼓励网络学习稀疏参数分布，使得剪枝变得更容易，因为可以安全移除接近于零的权重。

- 权重的重要性：
  - 通过权重的大小、权重对损失函数的梯度、或者权重对输入的激活情况评估权重重要性，来决定是否需要剪枝

## 何为剪枝

**剪枝类型**：包括非结构化剪枝、结构化剪枝和半结构化剪枝。

- **非结构化剪枝**：移除单个权重，导致稀疏但可能破坏结构。
- **结构化剪枝**：移除整体结构（如神经元或卷积核），保留模型结构，易于加速。
- **半结构化剪枝**：部分移除结构，兼顾性能和实现难度。

**剪枝范围：**包括局部剪枝和全局剪枝

- 局部剪枝：
  - 关注模型中的单个权重或参数。
  - 针对每个权重进行评估，决定是否将其设置为零。
  - 目的是移除对模型输出影响较小的权重。
  - 可以是权重剪枝、神经元剪枝或通道剪枝（例如，在卷积神经网络中移除整个卷积通道）。对模型的每个部分进行独立操作，不依赖于模型的其他部分。
- 全局剪枝：
  - 考虑模型的整体结构和性能。
  - 可能会移除整个神经元、卷积核、层或更复杂的结构。
  - 目标是优化整个模型的性能，同时减少模型的复杂度。
  - 通常需要对模型的整体结构有深入理解，可能涉及模型架构的重设计。对模型的最终性能影响较大，因为它改变了模型的整体特征提取能力。

**剪枝粒度**：粒度可分为细粒度剪枝（Fine-grained Pruning）、基于模式的剪枝（Pattern-based Pruning）、向量级剪枝（Vector-level Pruning）、内核级剪枝（Kernel-level Pruning）与通道级剪枝（Channel-level Pruning）

- **细粒度剪枝** (Fine-grained Pruning)：
  - 通过移除权重矩阵中的任意值进行剪枝。
  
  - 可以实现高压缩比，但对硬件支持不友好，速度增益有限。
  
- **基于模式的剪枝** (Pattern-based Pruning)：
  - 采用N:M稀疏性，即在每M个连续权重中固定有N个非零值，其余元素为零。
  
  - 这种方法可以利用特定硬件（ 比如英伟达安培A100 配备了 Sparse Tensor Cores，可加速 2:4 结构的细粒度稀疏性，充分利用了网络权重的细粒度稀疏性）加速矩阵乘法。
  
- **向量级剪枝** (Vector-level Pruning)：
  - 以行或列为单位对权重进行裁剪。

- **内核级剪枝** (Kernel-level Pruning)：
  - 以卷积核（滤波器）为单位进行权重裁剪。
- **通道级剪枝** (Channel-level Pruning)：
  - 以通道为单位对权重进行裁剪，通常改变网络中的滤波器组和特征通道数目。
  - 这种方法属于结构化剪枝，能够在不需要专门算法设计的情况下运行。

细粒度和基于模式的剪枝通常更灵活，但实现上更复杂，而通道级剪枝则更易于在现有硬件上高效运行。

## 剪枝标准

- **基于权重大小**：

  - 直接根据权重的绝对值大小来评估重要性。假设权重绝对值越小，其对模型输出的影响越小，因此可以安全移除。

  - 该方法可以分为：
    - **L1元素剪枝**：移除绝对值较小的单个权重。
      - **L1行剪枝**：计算每行权重的绝对值之和，移除重要性较小的行。
      - **L2剪枝**：计算每行权重的平方和的平方根，移除重要性较小的行。

- **基于梯度大小**：
  - 通过分析权重的梯度来判断其重要性。较大的梯度表示该权重对输出损失的影响较大，因此更重要；反之，较小的梯度表示该权重可以被剪除。

- **基于尺度**：
  - 该方法主要在卷积层中实现通道级的稀疏性，利用批归一化层中的缩放因子来评估通道的重要性。通过L1正则化鼓励不重要的通道的缩放因子趋向于零，从而可以被剪除。

- **基于二阶导数**：
  - 例如最优脑损伤（Optimal Brain Damage, OBD）方法，通过计算损失函数相对于权重的Hessian矩阵来评估权重的重要性。根据Hessian矩阵的特征值，判断哪些权重可以被剪除。
  - OBD公式推导：https://datawhalechina.github.io/llm-deploy/#/chapter3/chapter3_2_1

## 剪枝频率

主要分为两种类型：**迭代剪枝**和**单次剪枝**。

- **迭代剪枝**（Iterative Pruning）：
  - 这种方法是渐进式的，涉及多个循环的剪枝和微调步骤。
    - 过程包括：
      - 训练一个完整的模型。
      - 轻微剪枝，移除一小部分权重。
      - 微调剪枝后的模型，以恢复性能。
      - 在验证集上评估性能，并重复剪枝和微调，直到达到预定标准或剪枝比例。
  
  - 优点是可以细致评估每次剪枝对模型性能的影响，允许模型调整剩余权重。
  
- **单次剪枝**（One-Shot Pruning）：
- 在训练完成后进行一次性的剪枝操作。
  
- 特点是高效直接，不需要多次迭代。
  
- 模型在训练后根据剪枝标准（如权重绝对值）确定哪些参数可以被移除。
  
- 这种方法可能受到噪声影响较大，但对于大模型来说，由于微调成本高，通常更倾向于使用单次剪枝。

## 剪枝时机

**训练后剪枝**、**训练时剪枝**和**训练前剪枝**。

- **训练后剪枝**：

  - 基本思想：先训练一个完整的模型，然后对其进行剪枝，最后微调剪枝后的模型。
    - 步骤：
      - 初始训练：使用标准方法训练神经网络。
      - 识别重要连接：根据权重大小识别重要的神经连接。
      - 设置阈值：确定剪枝的阈值，移除低于该阈值的权重。
      - 剪枝和微调：剪除不重要的连接后，重新训练模型以调整剩余连接的权重。

  - 优点：稳定性高，适合特定任务的微调。

- **训练时剪枝**：

  - 基本思想：在模型训练过程中动态进行剪枝，允许权重适应并可能重新激活。
    - 步骤：
      - 初始化模型参数并进行训练。
      - 在每个训练周期结束时计算每个连接的重要性。
      - 根据设定的剪枝率选择并剪除不重要的连接。
      - 继续训练并调整权重。

  - 优点：能够更早地修剪不必要的连接，减少内存和计算需求。

- **训练前剪枝**：

  - 基本思想：在模型训练前进行剪枝，确定哪些连接不重要。
    - 步骤：
      - 随机选择连接进行剪枝，创建稀疏网络架构。
      - 重新训练剪枝后的网络。

  - 优点：可以加快训练速度，减少初始模型大小。

**训练后剪枝**适合稳定性要求高的场景，

**训练时剪枝**能有效减少计算需求，

**训练前剪枝**则加快了训练过程。

## 剪枝比例

主要涉及如何在神经网络的不同层之间分配剪枝率，通常分为两种方法：

- **均匀分层剪枝**（Uniform Layer-Wise Pruning）：
  - 在每一层应用相同的剪枝率。
  
  - 实现简单，剪枝率易于控制。
  
  - 但忽略了各层对模型整体性能的重要性差异，可能导致性能下降。
  
- **非均匀分层剪枝**（Non-Uniform Layer-Wise Pruning）：
- 根据每一层的特性分配不同的剪枝率。
  
- 可以基于梯度信息、权重大小或其他指标（如信息熵、Hessian矩阵）来确定。
  
- 对于重要层保留更多参数，而不重要的层则可以剪得更多。
  
- 通常能提供比均匀剪枝更好的性能。

通过合理分配剪枝比例，可以在保持模型性能的同时有效减少模型的复杂性和计算需求。